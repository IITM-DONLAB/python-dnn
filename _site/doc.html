<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Python-DNN</title>

  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->

  <link href="https://netdna.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css" rel="stylesheet">
  <link href="/assets/css/docs.min.css" rel="stylesheet">
  <link href="/assets/css/style.css" rel="stylesheet">

  <style>
    #content {
      background:#080331;
      background:linear-gradient(135deg, #080331, #673051);
    }
  </style>

  <script src="https://code.jquery.com/jquery-1.11.2.min.js"></script>
  <script src="https://netdna.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/bootbox.js/4.3.0/bootbox.min.js"></script>
  <script src="/assets/js/docs.min.js"></script>
</head>

<body>
  <!-- Main navigation
  =========================================== -->
  <header class="navbar navbar-static-top bs-docs-nav" id="top">
    <div class="container">
      <div class="navbar-header" role="banner">
        <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".bs-navbar-collapse">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="/index.html">Python-DNN</a>
      </div>
      <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation">
        <ul class="nav navbar-nav">
        
        
          
          <li class="active">
            <a href="/doc.html">Configuration</a>
          </li>
          
        
          
        
          
        
        </ul>
        <ul class="nav navbar-nav navbar-right">
        
          <li>
            <a href="https://github.com/IITM-DONLAB/python-dnn">GitHub</a>
          </li>
        
        </ul>
      </nav>
    </div>
  </header>


  <!-- Header
  =========================================== -->
  
  <div class="bs-docs-header" id="content">
    <div class="container">
      <h1>Configuration</h1>
      <p>Python-DNN Configuration Documentation</p>
    </div>
  </div>
  

  <div class="container bs-docs-container">
    <div class="row">
      <!-- Content
      =========================================== -->
      <div class="col-md-9" role="main">
        <section class="bs-docs-section">
          <!--<h1 id="overview" class="page-header"></h1>-->
          <div>
  <h1 id=main>Overview</h1>
  Python-DNN uses 3 configuration files which are in json format.
  <ol>
    <li> Model Configuration (model.conf) </li>
    <li> Data Configuration (data.conf) </li>
    <li> Network Configuration (nnet.conf)</li>
  </ol>
  </div>
</section>

<section class="bs-docs-section">
  
  <h1 id="model-configuration">Model Configuration</h1>

<ul>
  <li><code class="highlighter-rouge">nnetType</code> : (Mandatory) Type of Network (CNN/RBM/SDA/DNN)</li>
  <li><code class="highlighter-rouge">train_data</code> : (Mandatory) The working directory containing data configuration and output</li>
  <li><code class="highlighter-rouge">wdir</code> : (Mandatory) Working Directory.</li>
  <li><code class="highlighter-rouge">data_spec</code> : (Mandatory) The path of the data sepification relative to <code class="highlighter-rouge">model_config.json</code></li>
  <li>
    <p><code class="highlighter-rouge">nnet_spec</code> : (Mandatory) The path of network configuration specification relative to <code class="highlighter-rouge">model_config.json</code></p>
  </li>
  <li><code class="highlighter-rouge">output_file</code> : (Mandatory) The path of RBM network output file relative to <code class="highlighter-rouge">wdir</code></li>
  <li>
    <p><code class="highlighter-rouge">input_file</code> : The path of PreTrained/FineTuned network input file relative to <code class="highlighter-rouge">wdir</code>.(Mandatory for DNN)</p>
  </li>
  <li><code class="highlighter-rouge">random_seed</code> : Random Seed used for  initialization of weights.</li>
  <li>
    <p><code class="highlighter-rouge">logger_level</code> : Level of Logger.Valid Values are “INFO”,”DEBUG” and “ERROR”</p>
  </li>
  <li><code class="highlighter-rouge">batch_size</code> : specify the mini batch size while training, default 128</li>
  <li><code class="highlighter-rouge">n_ins</code> :Dimension of input (Mandatory for all except CNN)</li>
  <li><code class="highlighter-rouge">n_outs</code> :(Mandatory) Dimension of output (No: of Classes)</li>
  <li>
    <p><code class="highlighter-rouge">input_shape</code>: The input shape of a given feature vector.(Mandatory For CNN).Should be an Array.</p>
  </li>
  <li><code class="highlighter-rouge">finetune_params</code> : Configuration of finetune learning method.Contains a json object with following params:</li>
</ul>

<blockquote>
  <ul>
    <li><code class="highlighter-rouge">momentum</code> :  The momentum factor while finetuning</li>
    <li><code class="highlighter-rouge">method</code> :  Two methods are supported</li>
  </ul>
</blockquote>

<blockquote>
  <blockquote>
    <ol>
      <li>C: <strong>Constant learning rate</strong>(DEFAULT): run <code class="highlighter-rouge">epoch_num</code> iterations with <code class="highlighter-rouge">learning_rate</code> unchanged</li>
      <li>E: <strong>Exponential decay</strong>: we start with the learning rate of <code class="highlighter-rouge">start_rate</code>; if the validation error reduction between two epochs is less than <code class="highlighter-rouge">min_derror_decay_start</code>, the learning rate is scaled by <code class="highlighter-rouge">scale_by</code> during each of the remaining epoch. The whole traing terminates when the validation error reduction between two epochs falls below <code class="highlighter-rouge">min_derror_stop</code>. <code class="highlighter-rouge">min_epoch_decay_star</code> is the minimum epoch number after which scaling can only be performed.</li>
    </ol>
  </blockquote>
</blockquote>

<blockquote>
  <p>Default value of other paramters</p>
</blockquote>

<blockquote>
  <table>
    <thead>
      <tr>
        <th style="text-align: left">Param</th>
        <th style="text-align: center">Default value</th>
        <th style="text-align: center">Learning method</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left"><code class="highlighter-rouge">learning_rate</code></td>
        <td style="text-align: center">0.08</td>
        <td style="text-align: center">C</td>
      </tr>
      <tr>
        <td style="text-align: left"><code class="highlighter-rouge">epoch_num</code></td>
        <td style="text-align: center">10</td>
        <td style="text-align: center">C</td>
      </tr>
      <tr>
        <td style="text-align: left"><code class="highlighter-rouge">start_rate</code></td>
        <td style="text-align: center">0.08</td>
        <td style="text-align: center">E</td>
      </tr>
      <tr>
        <td style="text-align: left"><code class="highlighter-rouge">scale_by</code></td>
        <td style="text-align: center">0.5</td>
        <td style="text-align: center">E</td>
      </tr>
      <tr>
        <td style="text-align: left"><code class="highlighter-rouge">min_derror_decay_start</code></td>
        <td style="text-align: center">0.05</td>
        <td style="text-align: center">E</td>
      </tr>
      <tr>
        <td style="text-align: left"><code class="highlighter-rouge">min_derror_stop</code></td>
        <td style="text-align: center">0.05</td>
        <td style="text-align: center">E</td>
      </tr>
      <tr>
        <td style="text-align: left"><code class="highlighter-rouge">min_epoch_decay_start</code></td>
        <td style="text-align: center">15</td>
        <td style="text-align: center">E</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>These parameters are used by <strong>Constant learning rate</strong> or <strong>Exponential decay</strong></p>
</blockquote>

<ul>
  <li><code class="highlighter-rouge">pretrain_params</code>: Configuration of pretraining method.Contains a json object with following params</li>
</ul>

<blockquote>
  <table>
    <thead>
      <tr>
        <th>Param</th>
        <th style="text-align: center">Default value</th>
        <th style="text-align: center">nnet Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code class="highlighter-rouge">gbrbm_learning_rate</code></td>
        <td style="text-align: center">0.005</td>
        <td style="text-align: center">DBN</td>
        <td>Pretraining learning rate for gbrbm layer.</td>
      </tr>
      <tr>
        <td><code class="highlighter-rouge">learning_rate</code></td>
        <td style="text-align: center">0.08</td>
        <td style="text-align: center">SDA,DBN</td>
        <td>Pretraining learning rate (DBN: for all layers except gbrbm layer)</td>
      </tr>
      <tr>
        <td><code class="highlighter-rouge">epochs</code></td>
        <td style="text-align: center">15</td>
        <td style="text-align: center">DBN</td>
        <td>No of Pretraining epochs</td>
      </tr>
      <tr>
        <td><code class="highlighter-rouge">initial_momentum</code></td>
        <td style="text-align: center">0.5</td>
        <td style="text-align: center">DBN</td>
        <td>The initial momentum factor while pre-training</td>
      </tr>
      <tr>
        <td><code class="highlighter-rouge">final_momentum</code></td>
        <td style="text-align: center">0.9</td>
        <td style="text-align: center">DBN</td>
        <td>The final momentum factor while pre-training</td>
      </tr>
      <tr>
        <td><code class="highlighter-rouge">initial_momentum_epoch</code></td>
        <td style="text-align: center">5</td>
        <td style="text-align: center">DBN</td>
        <td>No: of epochs with the initial momentum factor before switching to final momentum factor</td>
      </tr>
      <tr>
        <td><code class="highlighter-rouge">keep_layer_num</code></td>
        <td style="text-align: center">0</td>
        <td style="text-align: center">SDA,DBN</td>
        <td>From which layer Pre-Trainig Should Start.If non-Zero layer is intilaized with weights from <code class="highlighter-rouge">input_file</code></td>
      </tr>
    </tbody>
  </table>
</blockquote>

<ul>
  <li><code class="highlighter-rouge">export_path</code> : path (realative to <code class="highlighter-rouge">wdir</code>) for writting (bottleneck) features.</li>
  <li><code class="highlighter-rouge">processes</code> : Process should be run by program.Contains a json object with following params</li>
</ul>

<blockquote>
  <ul>
    <li><code class="highlighter-rouge">pretraining</code> : whether Pre-Training is needed.(invalid for DNN and CNN).(Default value = false)</li>
    <li><code class="highlighter-rouge">finetuning</code> : whether Fine Tuning  is needed.(Default value = false)</li>
    <li><code class="highlighter-rouge">testing</code> : whether Fine Tuning  is needed.(Default value = false)</li>
    <li><code class="highlighter-rouge">export_data</code> : whether extracted features should written to file.If true,<code class="highlighter-rouge">export_path</code> is required.(Default value = false).</li>
  </ul>
</blockquote>

<hr />

<p><strong>Also See</strong>:</p>

<ul>
  <li><a href="https://github.com/IITM-DONLAB/python-dnn/tree/master/sample_config/MNIST/CNN/model_config.json">Example-CNN</a></li>
  <li><a href="https://github.com/IITM-DONLAB/python-dnn/tree/master/sample_config/MNIST/DBN/model_config.json">Example-RBM</a></li>
  <li><a href="https://github.com/IITM-DONLAB/python-dnn/tree/master/sample_config/MNIST/SDA/model_config.json">Example-SDA</a></li>
</ul>

</section>

<section class="bs-docs-section">
  <div>
    
    <h1 id="data-configuration">Data Configuration</h1>

<p>Data Specification has 3 fields:</p>

<blockquote>
  <ol>
    <li><code class="highlighter-rouge">training</code></li>
    <li><code class="highlighter-rouge">validation</code></li>
    <li><code class="highlighter-rouge">testing</code></li>
  </ol>
</blockquote>

<p>Each one is a json object with following fields:</p>

<blockquote>
  <ul>
    <li><code class="highlighter-rouge">base_path</code> :(Mandatory) Base path of data.</li>
    <li><code class="highlighter-rouge">filename</code> :(Mandatory) Filename,</li>
    <li><code class="highlighter-rouge">partition</code> :(Mandatory) Size of data which should be loaded to memory at a time (in MiB)</li>
    <li><code class="highlighter-rouge">random</code> : Whether to use random order (Default value = true)</li>
    <li><code class="highlighter-rouge">random_seed</code> : Seed for random numbers if <code class="highlighter-rouge">random</code> is <code class="highlighter-rouge">true</code></li>
    <li><code class="highlighter-rouge">keep_flatten</code> : Whether to use data as flatten vector or reshape(Default Value = false)</li>
    <li><code class="highlighter-rouge">reader_type</code> : (Mandatory) Type of reader NP/T1/T2.</li>
    <li><code class="highlighter-rouge">dim_shuffle</code> : how to use reshape given fatten vector.Used only <code class="highlighter-rouge">keep_flatten</code> is <code class="highlighter-rouge">false</code></li>
  </ul>
</blockquote>

<hr />

<p><strong>Also See</strong>:</p>

<ul>
  <li><a href="https://github.com/IITM-DONLAB/python-dnn/tree/master/sample_config/MNIST/CNN/data_spec.json">Example</a></li>
  <li><a href="#data-file-formats">Reader Type and Data Formats</a></li>
</ul>

  </div>
</section>

<section class="bs-docs-section">
  <h1 id="nnet_spec" class="page-header">NNET Configuration</h1>
  <div>
    
    <h2 id="cnn-specification">CNN Specification</h2>

<p>It has 2 parts:</p>

<blockquote>
  <ol>
    <li><code class="highlighter-rouge">cnn</code></li>
    <li><code class="highlighter-rouge">mlp</code></li>
  </ol>
</blockquote>

<p>Each one contain a json object.<code class="highlighter-rouge">cnn</code> describes convolution layer configuration and <code class="highlighter-rouge">mlp</code> describes hidden layer configuration.</p>

<ul>
  <li><code class="highlighter-rouge">cnn</code> contains a json object with following parameters:</li>
</ul>

<blockquote>
  <ul>
    <li><code class="highlighter-rouge">layers</code>: An Array of json objects.Each one decribes a convolution layer which contains:
&gt; * <code class="highlighter-rouge">convmat_dim</code> : Dimension of Convolution Weight
&gt; * <code class="highlighter-rouge">num_filters</code> : No. of Feature maps
&gt; * <code class="highlighter-rouge">poolsize</code>    : Dimension for Max-pooling
&gt; * <code class="highlighter-rouge">flatten</code>     : whether to flatten output or not(true for last layer else false)
&gt; * <code class="highlighter-rouge">update</code>      : true if weight need to updated during training.
&gt; * <code class="highlighter-rouge">activation</code>  : Activation function used by this layer, if not present global activation fuction is used.</li>
  </ul>
</blockquote>

<blockquote>
  <ul>
    <li><code class="highlighter-rouge">activation</code> : Activation function used by layers (global)</li>
    <li><code class="highlighter-rouge">use_fast</code> : if true program will use pylearn2 library for faster computation (Default Value = false)</li>
  </ul>
</blockquote>

<ul>
  <li><code class="highlighter-rouge">mlp</code> contains a json object with following parameters:</li>
</ul>

<blockquote>
  <ul>
    <li><code class="highlighter-rouge">layers</code>        : An Array contain size of hidden layers.</li>
    <li><code class="highlighter-rouge">adv_activation</code>: if maxout/pnorm is used.
&gt; * <code class="highlighter-rouge">method</code> : ‘maxout’,’pnorm’.
&gt; In <code class="highlighter-rouge">maxout</code>, a pooling of neuron o/p is done based on poolsize.
&gt; But in <code class="highlighter-rouge">pnorm</code> output is normalized after pooling.
&gt; * <code class="highlighter-rouge">pool_size</code>: pool size
&gt; * <code class="highlighter-rouge">pnorm_order</code>: order of normalization (in pnorm)</li>
  </ul>
</blockquote>

<blockquote>
  <ul>
    <li><code class="highlighter-rouge">activation</code>    : Activation function used by layers. (if adv_activation is used, it sholud be either ‘linear’,’relu’ or ‘cappedrelu’)</li>
  </ul>
</blockquote>

<hr />
<p><strong>Also See</strong></p>

<ul>
  <li><a href="https://github.com/IITM-DONLAB/python-dnn/tree/master/sample_config/MNIST/CNN/nnet_spec.json">Example</a></li>
  <li><a href="#activation-functions">Types of Activation functions</a></li>
</ul>

  </div>
  <div>
  
  <h2 id="dbnrbm-specification">DBN(RBM) Specification</h2>

<ul>
  <li><code class="highlighter-rouge">hidden_layers</code> : (Mandatory) An Array contain size of hidden RBM layer.</li>
  <li><code class="highlighter-rouge">activation</code> : Activation function used by layers.</li>
  <li><code class="highlighter-rouge">pretrained_layers</code> : Number of layers to be pre-trained.(Default Value = Size of <code class="highlighter-rouge">hidden_layers</code>)</li>
  <li><code class="highlighter-rouge">first_layer_type</code> : Type for the first layer.It should be either ‘bb’ (Bernoulli-Bernoulli) or ‘gb’ (Gaussian-Bernoulli).(Default Value = gb).</li>
</ul>

<hr />
<p><strong>Also See</strong></p>

<ul>
  <li><a href="https://github.com/IITM-DONLAB/python-dnn/tree/master/sample_config/MNIST/DBN/rbm_spec.json">Example</a></li>
  <li><a href="#activation-functions">Types of Activation functions</a></li>
</ul>

  </div>
  <div>
  
  <h2 id="dnn-specification">DNN Specification</h2>

<ul>
  <li><code class="highlighter-rouge">hidden_layers</code> : (Mandatory) An Array contain size of hidden RBM layer.</li>
  <li>
    <p><code class="highlighter-rouge">pretrained_layers</code> : Number of layers to be pre-trained.(Default Value = Size of <code class="highlighter-rouge">hidden_layers</code>)</p>
  </li>
  <li><code class="highlighter-rouge">max_col_norm</code> : The max value of norm of gradients; usually used in dropout and maxout.(Default Value = null)</li>
  <li><code class="highlighter-rouge">l1_reg</code> : l1 norm regularization weight.(Default Value = 0)</li>
  <li>
    <p><code class="highlighter-rouge">l2_reg</code> : l2 norm regularization weight.(Default Value = 0)</p>
  </li>
  <li><code class="highlighter-rouge">adv_activation</code>: if maxout/pnorm is used. It contains</li>
</ul>

<blockquote>
  <ul>
    <li><code class="highlighter-rouge">method</code> : Either ‘maxout’,’pnorm’. In <code class="highlighter-rouge">maxout</code>, a pooling of neuron o/p is done based on poolsize. But in <code class="highlighter-rouge">pnorm</code> output is normalized after pooling.</li>
    <li><code class="highlighter-rouge">pool_size</code>:  The number of units in each max-pooling(or pnorm) group for maxout/pnorm (Default Value = 1)</li>
    <li><code class="highlighter-rouge">pnorm_order</code>: The norm order for pnorm.(Default Value = 1)</li>
  </ul>
</blockquote>

<ul>
  <li>
    <p><code class="highlighter-rouge">activation</code>    : Activation function used by layers. (if adv_activation is used, it sholud be either ‘linear’,’relu’ or ‘cappedrelu’)</p>
  </li>
  <li><code class="highlighter-rouge">do_dropout</code> : whether to use dropout or not. (Default Value =false)</li>
  <li><code class="highlighter-rouge">dropout_factor</code> : the dropout factors for DNN layers.(One for each hidden layer)(Default Value =[0.0])</li>
  <li><code class="highlighter-rouge">input_dropout_factor</code> : The dropout factor for the input features.(Default Value =0.0)</li>
</ul>

<hr />
<p><strong>Also See</strong></p>

<ul>
  <li><a href="https://github.com/IITM-DONLAB/python-dnn/tree/master/sample_config/MNIST/DNN/dnn_spec.json">Example</a></li>
  <li><a href="#activation-functions">Types of Activation functions</a></li>
</ul>

  </div>
  <div>
  
  <h2 id="sda-specification">SDA Specification</h2>

<ul>
  <li><code class="highlighter-rouge">hidden_layers</code> : (Mandatory) An Array contains size of hidden denoising autoencoder layers.</li>
  <li><code class="highlighter-rouge">activation</code> : Activation function used by layers</li>
  <li><code class="highlighter-rouge">corruption_levels</code> : (Mandatory) An Array contains corruption level for each layer.Size should be equal to size of <code class="highlighter-rouge">hidden_layers</code></li>
</ul>

<hr />
<p><strong>Also See</strong></p>

<ul>
  <li><a href="https://github.com/IITM-DONLAB/python-dnn/tree/master/sample_config/MNIST/SDA/sda_spec.json">Example</a></li>
  <li><a href="#activation-functions">Types of Activation functions</a></li>
</ul>

  </div>
</section>
<section class="bs-docs-section">
  
  <h1 id="activation-functions">Activation Functions</h1>

<p>Activation Function is a function used to transform the activation level of a unit (neuron) into an output signal. Typically, activation functions have a “squashing” effect.</p>

<p><em>Python-DNN</em> currently support following Activation Functions:</p>

<ul>
  <li><code class="highlighter-rouge">sigmoid</code>:</li>
</ul>

<blockquote>
  <p>Sigmoid function with equation: f(x) = 1/(1 + e^(-x)).This is an S-shaped (sigmoid) curve, with output in the range (0,1).</p>
</blockquote>

<ul>
  <li><code class="highlighter-rouge">tanh</code>:</li>
</ul>

<blockquote>
  <p>The Hyperbolic tangent function is a sigmoid curve, like the logistic function, except that output lies in the range (-1,+1).</p>
</blockquote>

<ul>
  <li><code class="highlighter-rouge">relu</code>:</li>
</ul>

<blockquote>
  <p>The rectifier is an activation function defined as f(x) = max(0, x)</p>
</blockquote>

<ul>
  <li><code class="highlighter-rouge">cappedrelu</code>:</li>
</ul>

<blockquote>
  <p>It is same as ReLU except we cap the units at 6.ie, f(x) = min(max(x,0),6)</p>
</blockquote>

</section>

<section class="bs-docs-section">
  <div>
    
    <h1 id="data-file-formats">Data File Formats</h1>

<p><strong>Python-dnn</strong> supports 3 file formats* for data:</p>

<ol>
  <li><strong><em>NP</em></strong> :Numpy Format.</li>
  <li><strong><em>T1</em></strong> :Text File With One level header structure.</li>
  <li><strong><em>T2</em></strong> :Text File With Two level header structure</li>
</ol>

<hr />

<h3 id="numpy-format">Numpy Format</h3>

<p>The dataset is stored as single file in binary format
Data file has:</p>

<p><code class="highlighter-rouge">
	&lt;json-header&gt;
	&lt;structured numpy.array&gt;
	&lt;structured numpy.array&gt;
	..
	..
..
</code></p>

<p>Fist Line is a <strong>json-header</strong> which Contains two paramters:
&gt; * featdim : Dimention of input vector after flattening.
&gt; * input_shape : Actual shape of input before flattening.
&gt;
&gt; eg: <code class="highlighter-rouge"><span class="p">{</span><span class="nt">"featdim"</span><span class="p">:</span><span class="w"> </span><span class="mi">784</span><span class="p">,</span><span class="w"> </span><span class="nt">"input_shape"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">28</span><span class="p">,</span><span class="w"> </span><span class="mi">28</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">]}</span></code></p>

<p>Thereafter, each row contain a numpy structured Arrays (or Record Arrays) with vector as <code class="highlighter-rouge">'d'</code> and label as  <code class="highlighter-rouge">'l'</code>.The “dtype” of this Record Arrays is given by:<br />
&gt; <code class="highlighter-rouge">dtype={'names': ['d','l'],'formats': [('&gt;f2',header['featdim']),'&gt;i2']}</code></p>

<h3 id="text-file-one-level-header">Text File (One level header)</h3>

<p>In this type,we use a file containing list of <strong>simple text file</strong> names,each corresponding to single class.It has Following Format:
<code class="highlighter-rouge">
	&lt;feat_dim&gt; &lt;num_classes&gt;
	&lt;data_file1&gt;
	&lt;data_file2&gt;
	..
	..
</code>
Where <code class="highlighter-rouge">&lt;feat_dim&gt;</code> is Dimention of input vector and <code class="highlighter-rouge">&lt;num_classes&gt;</code> is No. of classes, <code class="highlighter-rouge">&lt;data_file1&gt;</code> is ‘simple text file’ of class 1,<code class="highlighter-rouge">&lt;data_file2&gt;</code> is ‘simple text file’ of class2 and so on.</p>

<p>The <strong>simple text file</strong> has the structure.
<code class="highlighter-rouge">
	&lt;feat_dim&gt; &lt;num_feat_vectors(optional)&gt;
	&lt;feat_vector&gt;
	&lt;feat_vector&gt;
	..
	..
</code>
Where <code class="highlighter-rouge">&lt;feat_dim&gt;</code> is Dimention of input vector,<code class="highlighter-rouge">&lt;num_feat_vectors&gt;</code> is No. of Vectors in this file and
<code class="highlighter-rouge">&lt;feat_vector&gt;</code> are features of a vector seperated by spaces.Whole file contains only feature vector of single class.</p>

<h3 id="text-file-two-level-header">Text File (Two level header)</h3>

<p>In this type,we use a file containing list of filesname with each with a list of <strong>simple text file</strong> names corresponding to a single class.It has Following Format:
<code class="highlighter-rouge">
	&lt;feat_dim&gt; &lt;num_classes&gt;
	&lt;class_index_file1&gt;
	&lt;class_index_file2&gt;
	..
	..
</code>
Where <code class="highlighter-rouge">&lt;feat_dim&gt;</code> is Dimention of input vector and <code class="highlighter-rouge">&lt;num_classes&gt;</code> is No. of classes, <code class="highlighter-rouge">&lt;class_index_file1&gt;</code> is ‘class index file’ (File with list of files of a class) of class 1,<code class="highlighter-rouge">&lt;class_index_file1&gt;</code> is ‘class index file’ of class2 and so on.</p>

<p>Each <code class="highlighter-rouge">&lt;class_index_file&gt;</code> is a file with Following Format:
<code class="highlighter-rouge">
	&lt;td_data_file1&gt;
	&lt;td_data_file2&gt;
	..
	..
</code>
Each <code class="highlighter-rouge">&lt;td_data_file*&gt;</code> is name of a  <strong>simple text file</strong>.The <strong>simple text file</strong> has the structure same as that of T1.</p>

<hr />

<p>*READER TYPE in <code class="highlighter-rouge">data_spec</code></p>

  </div>
</section>

<section class="bs-docs-section">
<h1 id="examples">Example</h1>
See: <a href="https://github.com/IITM-DONLAB/python-dnn/tree/master/sample_config/">Sample Configuration Folder</a>
</section>

<section>
<!-- to balance-->

        </section>
      </div>

      <!-- Page navigation
      =========================================== -->
      <div class="col-md-3" role="complementary">
        <div class="bs-docs-sidebar hidden-print">
          <ul class="nav bs-docs-sidenav">

          </ul>
          <a class="back-to-top" href="#top">
            <i class="glyphicon glyphicon-chevron-up"></i> Back to top
          </a>
        </div>
      </div>
    </div>
  </div>


  <!-- Footer
  =========================================== -->
  <footer class="bs-docs-footer" role="contentinfo">
    <div class="container">
      <div class="bs-docs-social">
        <ul class="bs-docs-social-buttons">
        
          <li>
            <iframe class="github-btn" src="http://ghbtns.com/github-btn.html?user=IITM-DONLAB&amp;repo=python-dnn&amp;type=watch&amp;count=true" width="90" height="20" title="Star on GitHub"></iframe>
          </li>
          <li>
            <iframe class="github-btn" src="http://ghbtns.com/github-btn.html?user=IITM-DONLAB&amp;repo=python-dnn&amp;type=fork&amp;count=true" width="90" height="20" title="Fork on GitHub"></iframe>
          </li>
        
        
          <li>
            <a href="https://twitter.com/share" class="twitter-share-button" data-url="" data-count="horizontal" data-via="" data-text="Python-DNN" data-hashtags="">Tweet</a>
          </li>
        
        
        
          <li>
            <div class="fb-like" data-href="" data-layout="button_count" data-action="like" data-show-faces="false" data-share="true"></div>
          </li>
        
        
        </ul>
      </div>

      <p>
        Licensed under <a href="http://www.apache.org/licenses/LICENSE-2.0" target="_blank">Apache License</a>,
        documentation under <a href="http://creativecommons.org/licenses/by/3.0/">CC BY 3.0</a>.
      </p>
      <p>
        Documentation template by <a href="http://getbootstrap.com">Bootstrap team</a>,
        generated with <a href="https://github.com/mistic100/jekyll-bootstrap-doc">Jekyll Bootstrap Doc</a>
      </p>

      <ul class="bs-docs-footer-links muted">
        <li>Currently v1.0.1</li>
      
        <li>&middot;</li>
        <li><a href="https://github.com/IITM-DONLAB/python-dnn">GitHub</a></li>
      
        <li>&middot;</li>
        <li><a href="https://github.com/IITM-DONLAB/python-dnn/issues?state=open">Issues</a></li>
      
      </ul>
    </div>
  </footer>


  <!-- Async scripts
  =========================================== -->
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.4.11/d3.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/trianglify/0.1.2/trianglify.min.js"></script>
  <script>trianglify('#080331', '#673051');</script>
  

  
  <script>
    window.twttr = (function (d,s,id) {
      var t, js, fjs = d.getElementsByTagName(s)[0];
      if (d.getElementById(id)) return; js=d.createElement(s); js.id=id; js.async=1;
      js.src="https://platform.twitter.com/widgets.js"; fjs.parentNode.insertBefore(js, fjs);
      return window.twttr || (t = { _e: [], ready: function(f){ t._e.push(f) } });
    }(document, "script", "twitter-wjs"));
  </script>
  

  
  <div id="fb-root"></div>
  <script>(function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
      if (d.getElementById(id)) return;
      js = d.createElement(s); js.id = id;
      js.src = "https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.0";
      fjs.parentNode.insertBefore(js, fjs);
    }(document, 'script', 'facebook-jssdk'));
  </script>
  

  

</body>
</html>
